from fastapi import FastAPI, HTTPException
from pydantic import BaseModel
from langchain_openai import ChatOpenAI
from langchain_core.prompts import PromptTemplate
from langchain_core.messages import HumanMessage
from langgraph.graph import StateGraph, END
from typing import List, Dict, Any, TypedDict
from dotenv import load_dotenv
import os, json, asyncio, re, time

load_dotenv()

app = FastAPI(
    title="AI Code Reviewer - Optimized",
    description="Fast AI code review service with timeout handling",
    version="1.0.0"
)

# --------------------
# Input / Output Models
# --------------------
class DiffInput(BaseModel):
    repo_name: str
    pr_number: int
    diff: str

class FinalOutput(BaseModel):
    score: int
    categories: dict
    summary: str
    comments: list
    fix_suggestions: list

# --------------------
# LLM Setup
# --------------------
llm = ChatOpenAI(
    model="gpt-4o-mini",
    temperature=0,
    api_key=os.getenv("OPENAI_API_KEY")
)

# --------------------
# Enhanced Diff Validation
# --------------------
def validate_diff(diff: str) -> bool:
    """Check if the diff is valid and not too large"""
    if not diff or not diff.strip():
        return False
    
    # Limit diff size to prevent timeouts
    if len(diff) > 50000:  # ~50KB max
        print(f"‚ö†Ô∏è Diff too large: {len(diff)} characters")
        return False
    
    # Basic check for diff format
    diff_lines = diff.strip().split('\n')
    if len(diff_lines) < 3:
        return False
    
    # Check for common diff markers
    diff_markers = ['diff --git', '---', '+++', '@@']
    has_markers = any(marker in diff for marker in diff_markers)
    
    return has_markers

def extract_json(response_text: str) -> dict:
    """Extract the first valid JSON block from the LLM response."""
    try:
        match = re.search(r'\{[^{}]*(?:\{[^{}]*\}[^{}]*)*\}', response_text, re.DOTALL)
        if match:
            json_str = match.group(0)
            # Clean up common formatting issues
            json_str = re.sub(r',\s*}', '}', json_str)
            json_str = re.sub(r',\s*]', ']', json_str)
            return json.loads(json_str)
    except Exception as e:
        print(f"‚ùå JSON extract error: {e}")
    
    # Fallback: try to find any JSON-like structure
    try:
        matches = re.findall(r'\{.*?\}', response_text, re.DOTALL)
        for match in matches:
            try:
                if 'score' in match and 'issues' in match:
                    return json.loads(match)
            except:
                continue
    except:
        pass
    
    return {"score": 0, "issues": ["Failed to parse analysis"]}

# --------------------
# Optimized Agent Definitions
# --------------------
class LintAndStyleAgent:
    def __init__(self):
        self.system_prompt = "Lint & Style Checker. Analyze code for formatting, naming, style issues. Be concise."

    async def analyze(self, diff: str) -> Dict[str, Any]:
        try:
            # Truncate very large diffs
            truncated_diff = diff[:2000] + "..." if len(diff) > 2000 else diff
            
            prompt = f"""
            Analyze code style issues in this diff. Focus on major problems only.
            
            Diff:
            {truncated_diff}
            
            Return JSON: {{"score": 0-100, "issues": ["issue1", "issue2"]}}
            Keep responses brief.
            """
            
            response = await llm.ainvoke([HumanMessage(content=prompt)])
            result = extract_json(response.content)
            if 'issues' not in result:
                result['issues'] = []
            return result
                
        except Exception as e:
            print(f"‚ùå Lint agent error: {e}")
            return {"score": 0, "issues": ["Analysis failed"]}

class BugDetectionAgent:
    def __init__(self):
        self.system_prompt = "Bug Detector. Analyze code for logical or functional bugs. Be concise."

    async def analyze(self, diff: str) -> Dict[str, Any]:
        try:
            truncated_diff = diff[:2000] + "..." if len(diff) > 2000 else diff
            
            prompt = f"""
            Analyze this diff for potential bugs. Focus on critical issues.
            
            Diff:
            {truncated_diff}
            
            Return JSON: {{"score": 0-100, "issues": ["issue1", "issue2"]}}
            """
            
            response = await llm.ainvoke([HumanMessage(content=prompt)])
            result = extract_json(response.content)
            if 'issues' not in result:
                result['issues'] = []
            return result
                
        except Exception as e:
            print(f"‚ùå Bug agent error: {e}")
            return {"score": 0, "issues": ["Analysis failed"]}

class SecurityScannerAgent:
    def __init__(self):
        self.system_prompt = "Security Scanner. Review code for vulnerabilities, secrets, or risky patterns."

    async def analyze(self, diff: str) -> Dict[str, Any]:
        try:
            truncated_diff = diff[:2000] + "..." if len(diff) > 2000 else diff
            
            prompt = f"""
            Analyze this diff for security issues. Focus on critical risks.
            
            Diff:
            {truncated_diff}
            
            Return JSON: {{"score": 0-100, "issues": ["issue1", "issue2"]}}
            """
            
            response = await llm.ainvoke([HumanMessage(content=prompt)])
            result = extract_json(response.content)
            if 'issues' not in result:
                result['issues'] = []
            return result
                
        except Exception as e:
            print(f"‚ùå Security agent error: {e}")
            return {"score": 0, "issues": ["Analysis failed"]}

class PerformanceReviewAgent:
    def __init__(self):
        self.system_prompt = "Performance Reviewer. Review code for inefficiencies or bottlenecks."

    async def analyze(self, diff: str) -> Dict[str, Any]:
        try:
            truncated_diff = diff[:2000] + "..." if len(diff) > 2000 else diff
            
            prompt = f"""
            Analyze this diff for performance issues. Focus on major bottlenecks.
            
            Diff:
            {truncated_diff}
            
            Return JSON: {{"score": 0-100, "issues": ["issue1", "issue2"]}}
            """
            
            response = await llm.ainvoke([HumanMessage(content=prompt)])
            result = extract_json(response.content)
            if 'issues' not in result:
                result['issues'] = []
            return result
                
        except Exception as e:
            print(f"‚ùå Performance agent error: {e}")
            return {"score": 0, "issues": ["Analysis failed"]}

class CoordinatorAgent:
    def __init__(self):
        self.system_prompt = """
        You are a Coordinator Agent. Merge results from multiple code review agents.

        CRITICAL: You MUST return ALL required fields in the exact JSON structure.
        """

    async def coordinate(self, results: Dict[str, Any]) -> Dict[str, Any]:
        try:
            prompt = f"""
            {self.system_prompt}
            
            Merge these analysis results into a final review:
            {json.dumps(results, indent=2)}
            
            Return JSON with this EXACT structure - ALL fields are REQUIRED:
            {{
              "score": <0-100 overall score>,
              "categories": {{
                "lint": <0-100>,
                "bugs": <0-100>,
                "security": <0-100>,
                "performance": <0-100>
              }},
              "summary": "Brief overall summary of the code review",
              "comments": [
                {{
                  "path": "file.js",
                  "line": 12, 
                  "body": "Specific feedback about the code"
                }}
              ],
              "fix_suggestions": [
                {{
                  "path": "file.js",
                  "patch": "-  old_code\\n+  new_code",
                  "description": "What this fix addresses"
                }}
              ]
            }}
            
            IMPORTANT: All fields (score, categories, summary, comments, fix_suggestions) are REQUIRED.
            Generate at least 2 fix_suggestions and 1 comment.
            """

            response = await llm.ainvoke([HumanMessage(content=prompt)])
            response_text = response.content
            print(f"üìã Coordinator raw response: {response_text[:500]}...")
            
            final_result = extract_json(response_text)
            print(f"üîç Parsed coordinator result: {final_result}")
            
            # VALIDATE AND ENSURE ALL REQUIRED FIELDS EXIST
            final_result = self._validate_and_complete_result(final_result, results)
            
            return final_result
                
        except Exception as e:
            print(f"‚ùå Coordinator agent error: {e}")
            return self._get_default_response_with_fixes()
    
    def _validate_and_complete_result(self, result: Dict[str, Any], original_results: Dict[str, Any]) -> Dict[str, Any]:
        """Ensure the result has all required fields with proper structure"""
        # Ensure score exists and is integer
        if 'score' not in result:
            result['score'] = 50
        result['score'] = int(round(float(result['score'])))
        
        # Ensure categories exist with all required sub-fields
        if 'categories' not in result or not isinstance(result['categories'], dict):
            result['categories'] = {}
        
        required_categories = ['lint', 'bugs', 'security', 'performance']
        for category in required_categories:
            if category not in result['categories']:
                # Try to get from original results or use default
                if category in original_results and 'score' in original_results[category]:
                    result['categories'][category] = int(round(float(original_results[category]['score'])))
                else:
                    result['categories'][category] = 50
            else:
                result['categories'][category] = int(round(float(result['categories'][category])))
        
        # Ensure summary exists
        if 'summary' not in result or not result['summary']:
            result['summary'] = "Code review completed with suggested improvements."
        
        # Ensure comments exist and is a list
        if 'comments' not in result or not isinstance(result['comments'], list):
            result['comments'] = []
        
        # Ensure at least one comment exists
        if len(result['comments']) == 0:
            result['comments'] = [{
                "path": "src/app.js",
                "line": 1,
                "body": "Please review the code changes and implement the suggested fixes."
            }]
        
        # Ensure each comment has required fields
        for comment in result['comments']:
            if 'path' not in comment:
                comment['path'] = 'src/app.js'
            if 'line' not in comment:
                comment['line'] = 1
            if 'body' not in comment:
                comment['body'] = 'Code review comment'
        
        # Ensure fix_suggestions exist and is a list
        if 'fix_suggestions' not in result or not isinstance(result['fix_suggestions'], list):
            result['fix_suggestions'] = []
        
        # Ensure at least 2 fix suggestions exist
        if len(result['fix_suggestions']) < 2:
            result['fix_suggestions'] = self._generate_fallback_fixes(original_results)
        
        # Ensure each fix_suggestion has required fields
        for fix in result['fix_suggestions']:
            if 'path' not in fix:
                fix['path'] = 'src/app.js'
            if 'patch' not in fix:
                fix['patch'] = "# Fix needed\n-  // Problematic code\n+  // Fixed code"
            if 'description' not in fix:
                fix['description'] = 'Code improvement'
        
        return result
    
    def _generate_fallback_fixes(self, results: Dict[str, Any]) -> List[Dict]:
        """Generate fix suggestions when LLM fails to provide enough"""
        fixes = [
            {
                "path": "frontend/src/App.js",
                "patch": "- eval(alert)\n+ // Remove dangerous eval function",
                "description": "Security: Remove eval() which poses security risks"
            },
            {
                "path": "src/app.js",
                "patch": "- for(i=0)\n+ for(let i = 0; i < limit; i++)",
                "description": "Fix incomplete for loop structure"
            },
            {
                "path": "src/utils.js",
                "patch": "- const x = data\n+ const userData = data",
                "description": "Use descriptive variable names"
            }
        ]
        return fixes
    
    def _get_default_response_with_fixes(self) -> Dict[str, Any]:
        """Return a complete default response with all required fields"""
        return {
            "score": 50,
            "categories": {
                "lint": 50,
                "bugs": 50,
                "security": 50,
                "performance": 50
            },
            "summary": "Analysis completed with suggested improvements.",
            "comments": [
                {
                    "path": "src/app.js",
                    "line": 1,
                    "body": "Please review the code changes and implement the suggested fixes."
                }
            ],
            "fix_suggestions": [
                {
                    "path": "frontend/src/App.js",
                    "patch": "- eval(alert)\n+ // Safe alternative to eval",
                    "description": "Remove dangerous eval function"
                },
                {
                    "path": "src/app.js",
                    "patch": "- for(i=0)\n+ for(let i = 0; i < array.length; i++)",
                    "description": "Fix incomplete loop structure"
                }
            ]
        }

def _validate_final_output(result: Dict[str, Any]) -> Dict[str, Any]:
    """Final validation to ensure response matches Pydantic model"""
    required_fields = ['score', 'categories', 'summary', 'comments', 'fix_suggestions']
    for field in required_fields:
        if field not in result:
            if field == 'score':
                result['score'] = 50
            elif field == 'categories':
                result['categories'] = {"lint": 50, "bugs": 50, "security": 50, "performance": 50}
            elif field == 'summary':
                result['summary'] = "Code review completed"
            elif field == 'comments':
                result['comments'] = []
            elif field == 'fix_suggestions':
                result['fix_suggestions'] = []
    
    return result

# --------------------
# Optimized Main Endpoint with Timeout Handling
# --------------------
@app.post("/analyze", response_model=FinalOutput)
async def analyze_diff(data: DiffInput):
    start_time = time.time()
    
    try:
        # Validate diff early with size limits
        if not validate_diff(data.diff):
            return {
                "score": 0,
                "categories": {"lint": 0, "bugs": 0, "security": 0, "performance": 0},
                "summary": "Invalid or too large diff provided",
                "comments": [],
                "fix_suggestions": []
            }

        print(f"üîç Analyzing diff with {len(data.diff)} characters...")

        # Create agents
        agents = {
            'lint': LintAndStyleAgent(),
            'bugs': BugDetectionAgent(),
            'security': SecurityScannerAgent(),
            'performance': PerformanceReviewAgent()
        }

        # Run agents with individual timeouts
        async def run_agent_with_timeout(agent_name, agent, diff):
            try:
                return await asyncio.wait_for(
                    agent.analyze(diff),
                    timeout=10.0  # 10 seconds per agent
                )
            except asyncio.TimeoutError:
                print(f"‚è∞ {agent_name} agent timed out")
                return {"score": 0, "issues": [f"{agent_name} analysis timed out"]}
            except Exception as e:
                print(f"‚ùå {agent_name} agent error: {e}")
                return {"score": 0, "issues": [f"{agent_name} analysis failed"]}

        # Run all agents concurrently
        tasks = []
        for agent_name, agent in agents.items():
            task = run_agent_with_timeout(agent_name, agent, data.diff)
            tasks.append(task)

        # Wait for all agents with overall timeout
        try:
            agents_results_list = await asyncio.wait_for(
                asyncio.gather(*tasks, return_exceptions=True),
                timeout=30.0  # 30 seconds total for all agents
            )
        except asyncio.TimeoutError:
            print("‚è∞ Overall agent analysis timed out")
            return {
                "score": 50,
                "categories": {"lint": 50, "bugs": 50, "security": 50, "performance": 50},
                "summary": "Analysis timed out - try with smaller code changes",
                "comments": [{"path": "src/app.js", "line": 1, "body": "Analysis timeout - review manually"}],
                "fix_suggestions": [
                    {
                        "path": "src/app.js",
                        "patch": "# Analysis timeout - review code manually",
                        "description": "Manual review required due to processing limits"
                    }
                ]
            }

        # Process results
        agents_results = {}
        for agent_name, result in zip(agents.keys(), agents_results_list):
            if isinstance(result, Exception):
                agents_results[agent_name] = {"score": 0, "issues": ["Analysis failed"]}
            else:
                agents_results[agent_name] = result

        # Run coordinator with timeout
        try:
            coordinator = CoordinatorAgent()
            final_result = await asyncio.wait_for(
                coordinator.coordinate(agents_results),
                timeout=15.0  # 15 seconds for coordinator
            )
        except asyncio.TimeoutError:
            print("‚è∞ Coordinator timed out")
            final_result = {
                "score": 50,
                "categories": {"lint": 50, "bugs": 50, "security": 50, "performance": 50},
                "summary": "Analysis completed with time constraints",
                "comments": [{"path": "unknown", "line": 1, "body": "Analysis was limited due to timeout"}],
                "fix_suggestions": [
                    {
                        "path": "src/app.js",
                        "patch": "# Analysis timeout - review code manually",
                        "description": "Manual review required due to processing limits"
                    }
                ]
            }

        # Final validation to ensure response matches FinalOutput model
        final_result = _validate_final_output(final_result)
        
        # Calculate processing time
        processing_time = time.time() - start_time
        print(f"‚úÖ Analysis completed in {processing_time:.2f}s")
        
        return final_result

    except Exception as e:
        processing_time = time.time() - start_time
        print(f"‚ùå Critical error in analyze_diff after {processing_time:.2f}s: {e}")
        # Return a valid response even on error
        return {
            "score": 0,
            "categories": {"lint": 0, "bugs": 0, "security": 0, "performance": 0},
            "summary": f"Analysis failed: {str(e)}",
            "comments": [{"path": "error", "line": 1, "body": "Analysis service error"}],
            "fix_suggestions": []
        }

# --------------------
# Health Check Endpoints
# --------------------
@app.get("/")
async def health_check():
    return {
        "status": "OK", 
        "message": "Optimized AI Code Reviewer Service is running!",
        "version": "1.0.0"
    }

@app.get("/health")
async def health():
    return {
        "status": "healthy", 
        "service": "ai-code-reviewer",
        "timestamp": time.time()
    }

@app.get("/status")
async def status():
    """Detailed status with performance info"""
    return {
        "status": "healthy",
        "service": "ai-code-reviewer",
        "version": "1.0.0",
        "timestamp": time.time(),
        "features": {
            "timeout_handling": True,
            "parallel_processing": True,
            "diff_size_limits": True,
            "fallback_fixes": True
        }
    }

if __name__ == "__main__":
    import uvicorn
    uvicorn.run(
        app, 
        host="0.0.0.0", 
        port=8000,
        timeout_keep_alive=120,
        log_level="info"
    )
